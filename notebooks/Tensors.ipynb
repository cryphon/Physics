{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d518472b",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "## What is a Tensor\n",
    "\n",
    "A Tensor is a generalization of scalars, vectors, and matrices to higher dimensions.\n",
    "They can be thought of as multi-dimensional arrays that obey specific transofmation rules under a change of basis.\n",
    "\n",
    "There are some general types of Tensors, they can be defined based on rank.\n",
    "| Tensor Type | Rank | Description                     | Example                                        | Notation  |\n",
    "| ----------- | ---- | ------------------------------- | ---------------------------------------------- | --------- |\n",
    "| Scalar      | 0    | Single number (magnitude only)  | $a \\in \\mathbb{R}$                             | $a$       |\n",
    "| Vector      | 1    | Direction and magnitude         | $\\vec{v} \\in \\mathbb{R}^n$                     | $v^i$     |\n",
    "| Matrix      | 2    | Linear map or transformation    | $A \\in \\mathbb{R}^{m \\times n}$                | $M^{ij}$  |\n",
    "| Higher-Rank | >2   | Multi-dimensional relationships | $T \\in \\mathbb{R}^{i \\times j \\times k \\dots}$ | $T^{ijk}$ |\n",
    "\n",
    "Here we can see a representation of what that would look like in code.\n",
    "```python\n",
    "scalar = 5\n",
    "vector = np.array([1, 2, 3])\n",
    "matrix = np.array([1, 2], [3, 4])\n",
    "tensor3 = np.zeros((2, 3, 4))\n",
    "```\n",
    "\n",
    "\n",
    "## Tensor Notation and indexing\n",
    "\n",
    "Tensors are often written using index notation. For example, a matrix element is written as $A_{ij}$.\n",
    "\n",
    "\n",
    "#### Einstein Summation Convention\n",
    "In this notation, __repeated indices are implicitly summed over__. For example:\n",
    "$$\n",
    "    C_{ik} = A_{ij}B_{jk}\n",
    "$$\n",
    "\n",
    "Which means:\n",
    "```python\n",
    "C = np.einsum('ij,jk->ik', A, B)\n",
    "```\n",
    "This is quite useful, as this also works for higher-rank tensors too.\n",
    "\n",
    "\n",
    "## Covariant vs. Contravariant Components\n",
    "When working with coordinate transformations, tensors may have indices that __go up__ (superscript) or __go down__ (subscript). This distinction matters in curved spaces or non-Cartesian coordinates.\n",
    "- __Contravariant__ components $V^i$ transform oppositely to the basis vector.\n",
    "- __Covariant__ components $V^i$ tranform with the basis vectors.\n",
    "\n",
    "They relate through the __metric tensor__:\n",
    "$$\n",
    "    V_i = g_{ij}V^j (lowering)  \\\\\n",
    "    V^i = g^{ij}V_j (raising)\n",
    "$$\n",
    "These concepts are key in general relativity and differential geometry.\n",
    "\n",
    "## Tensor Transformation Law\n",
    "A tensor must transform according to specific riles when coordinates are changed.\n",
    "For a (1, 1) tensor $T^i_j$, the transformation law is:\n",
    "$$\n",
    "    T^{\\prime i}_{\\ \\ j} = \\frac{\\partial x^k}{\\partial x^{\\prime i}} \\frac{\\partial x^{\\prime j}}{\\partial x^l} T^l_{\\ k}\n",
    "$$\n",
    "\n",
    "This rule ensures that the __physical meaning of the tensor stays the same__, even if the coordinates change.\n",
    "In simple terms: the components may change but the _object_ the tensor describes does not.\n",
    "\n",
    "## Common Tensor examples\n",
    "\n",
    "#### Metric Tensor $g_{ij}$\n",
    "- Allows you to compute dot products, lengths, and angles.\n",
    "- In Euclidian space, its the identity $ g_{ij} = \\delta_{ij} $\n",
    "- In general relativity, it encoded the curvature of spacetime.\n",
    "\n",
    "#### Kronecker Delta $\\delta^i_j$\n",
    "- Identity tensor:\n",
    "$$\n",
    "    \\delta^i_j = \\begin{cases}\n",
    "    1 & \\text{if } i = j \\\\\n",
    "    0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "- acts like a matrix identity element\n",
    "\n",
    "\n",
    "#### Levita-Civita Symbol $\\epsilon_{ijk}$\n",
    "- Fully antisymmetric\n",
    "- Used to compute cross products and determinants\n",
    "$$\n",
    "\\epsilon_{ijk} =\n",
    "\\begin{cases}\n",
    "+1 & \\text{if } (i, j, k) \\text{ is an even permutation of } (1, 2, 3) \\\\\n",
    "-1 & \\text{if } (i, j, k) \\text{ is an odd permutation of } (1, 2, 3) \\\\\n",
    "\\ \\ 0 & \\text{if any two indices are equal}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "## Tensor Operations\n",
    "### 1. Contraction\n",
    "sum over repeated indices, reducing tensor rank\n",
    "$$\n",
    "    T^i_i = trace\n",
    "$$\n",
    "```python\n",
    "np.trace(np.eye(3)) # contraction example\n",
    "```\n",
    "### 2. Tensor product\n",
    "Combine tensors to create higher-rank tensors\n",
    "$$\n",
    "(T \\otimes S)^{kl}_{\\ \\ ij} = T^k_{\\ i} S^l_{\\ j}\n",
    "$$\n",
    "```python\n",
    "np.tensordot(T, S, axes=0)\n",
    "```\n",
    "### 3. Index Raising/Lowering\n",
    "Use the metric tensor to switch between covariant and contravariant indices.\n",
    "$$\n",
    "    V^i = g^{ij}V_j\n",
    "$$\n",
    "\n",
    "### 4. Symmetrization / Antisymmetrization\n",
    "Used in physical theories where certain quantities are symmetric or antisymmetric.\n",
    "\n",
    "$$\n",
    "T_{(ij)} = \\frac{1}{2}(T_{ij} + T_{ji}) \\quad \\text{(symmetric part)} \\\\\n",
    "T_{[ij]} = \\frac{1}{2}(T_{ij} - T_{ji}) \\quad \\text{(antisymmetric part)}\n",
    "$$\n",
    "\n",
    "\n",
    "## Summary\n",
    "- A tensor is a multilinear map that transforms predictably under coordinate changes.\n",
    "- Rank indicates dimensionality.\n",
    "- Covariant and contravariant indices describe how components shift.\n",
    "- The metric tensor helps relate different types of tensors.\n",
    "- Tensors generalize familiar mathematical tools to more abstract and powerful concepts.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
